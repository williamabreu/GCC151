{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Informação dos Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook extrai informações dos corpora compilados, utilizando recursos de níveis (morfo)sintático, de forma que:\n",
    "- Carrega e estrutura os dados dos corpora em dicionário\n",
    "- Exibe estatísticas acerca da estrutura construída\n",
    "- Implementa um sistema simples de perguntas e respostas sobre os corpora\n",
    "- Apresenta comentários acerca dos resultados obtidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações Iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, fazer o ajuste no path do Python para a raíz do projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento dos Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar a bliblioteca de normalização de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils.lexical.normalizer import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciar um objeto para fazer as normalizações necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar uma rotina para auxiliar o carregamento dos dados e evitar replicação de código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função ```load_sentences``` recebe como entrada a string indicando o path do diretório do córpus a ser usado, retornando a lista contendo todas as sentenças desse córpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(corpus_dir):   \n",
    "    all_sentences = []\n",
    "    for file_path in os.listdir(corpus_dir):\n",
    "        with open(corpus_dir + file_path) as fp:\n",
    "            for line in fp:\n",
    "                # line = normalizer.to_lowercase(line)\n",
    "                sentences = normalizer.tokenize_sentences(line)\n",
    "                # sentences = [normalizer.tokenize_words(sent) for sent in sentences]\n",
    "                all_sentences.extend(sentences)\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as Sentenças dos Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar todas as sentenças de cada córpus e mesclar tudo em uma só lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar cada um dos córpus e concatenar na lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.extend(load_sentences('data/corpora/tecnologia/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.extend(load_sentences('data/corpora/mercado/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.extend(load_sentences('data/corpora/telefonia/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.extend(load_sentences('data/corpora/saude/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de Informações "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar a biblioteca ```Syntax``` para fazer o processamento nos corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils.syntax.parser import Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciar um objeto para fazer o parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos transformar as senteças de cada corpus na seguinte estrutura de dados:\n",
    "```\n",
    "{\n",
    "    “verbo_lematizado_1”: [(Sujeito_1, Objeto_1), (Sujeito_2, None), ..., (Sujeito_n, Objeto_n)], \n",
    "    ... \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for sentence in sentences:\n",
    "    svo_list = parser.get_SVO(sentence)\n",
    "    for subj, verb, obj in svo_list:\n",
    "        if verb.lemma_ in data:\n",
    "            data[verb.lemma_].append((subj, obj))\n",
    "        else:\n",
    "            data[verb.lemma_]= [(subj, obj)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo abaixo está o resultado do dicionário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "pp.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estatísticas em cima do dicionário ```data```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Qual verbo tem a maior lista de sujeitos e objetos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greatest = [[len(data[d]), d] for d in data]\n",
    "greatest = max(greatest)\n",
    "greatest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Há algum verbo sem objetos? Mostre alguns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller = [[len(data[d]), d] for d in data]\n",
    "smaller = min(smaller)\n",
    "smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistema básico de perguntas e respostas utilizando o dicionário ```data```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar a biblioteca ```Morphosyntax``` para fazer o processamento nos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils.morphosyntax.tagger import Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciar um objeto para fazer a etiquetagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Tagger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input('Digite uma pergunta: ')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "tagged_text = nlp(question)\n",
    "verb = ''\n",
    "for token in tagged_text:\n",
    "    if token.pos_ == 'VERB':\n",
    "        if token.lemma_ in data.keys():\n",
    "            verb = token.lemma_\n",
    "try:\n",
    "    for text in data[verb]:\n",
    "        print('Resposta: {} {} {}'.format(text[0], verb, text[1]))\n",
    "except KeyError:\n",
    "    print('A base de dados do dicionário não consegue responder esta pergunta')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
